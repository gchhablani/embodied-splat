<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device">
  <meta property="og:title" content="EmbodiedSplat" />
  <meta property="og:description" content="Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device" />
  <meta property="og:url" content="https://gchhablani.github.io/embodied-splat/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="gaussian splatting, embodied AI, sim-to-real, navigation, image-goal navigation, 3D reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>EmbodiedSplat</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/4.0.0/model-viewer.min.js"></script>  <!-- <model-viewer> -->
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with
              Gaussian Splats from a Mobile Device</h1>
            <h3 class="title is-3" style="color: #555; font-weight: bold;">ICCV 2025</h3>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://gchhablani.github.io" target="_blank">Gunjan Chhablani</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://github.com/yebyyy" target="_blank">Xiaomeng Ye</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://zubairirshad.com" target="_blank">Muhammad Zubair Irshad</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://faculty.cc.gatech.edu/~zk15/" target="_blank">Zsolt Kira</a><sup>4</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1,2,4</sup>Georgia Tech, <sup>3</sup>Toyota Research Institute</span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/main.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/supp.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/gchhablani/embodied-splat-v1" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Dataset Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/gchhablani/embodied-splat" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2509.17430" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <!--
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>
-->
  <!-- End teaser video -->

  <!-- Teaser Image -->
<section class="section hero is-small">
    <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <img src="static/images/EmbodiedSplat_teaser.png" alt="EmbodiedSplat Teaser Image" class="is-centered">
          <p>
            The sim-to-real gap remains a bottleneck in the field of Embodied AI due to synthetic datasets lacking realism and real-world datasets being expensive to collect. Advancements in 3D scene representation, such as 3D Gaussian Splatting (GS), followed by DN-Splatter, enable efficient scene reconstruction from low-effort collects, and their potential for training and deploying robot policies remains largely unexplored. In this work, we introduce EmbodiedSplat, an efficient and cost-effective pipeline for bridging the sim-to-real gap in navigation with the creation of high-quality 3D simulations from low-cost iPhone captures using depth-aware 3D Guassian Splats (GS) and <a href="https://poly.cam">Polycam</a>. Our comprehensive evaluations on both simulation and real-world environments show that EmbodiedSplat substantially improves sim-to-real transfer thanks to the effectiveness of fine-tuning on high-fidelity reconstructions. Our in-depth analysis explores the relationship between reconstruction quality, pre-training scenes, downstream navigation performance, and training strategies. Our findings provide valuable insights into how reconstruction fidelity influences policy generalization and applicability. We provide an open-source codebase and dataset to facilitate further research in this domain and reproducibility of results.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

  <!-- End Teaser Image -->

  <!-- Paper abstract -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The field of Embodied AI predominantly relies on simulation for training and evaluation, often using either fully synthetic environments that lack photorealism or high-fidelity real-world reconstructions captured with expensive hardware. As a result, sim-to-real transfer remains a major challenge. In this paper, we introduce EmbodiedSplat, a novel approach that personalizes policy training by efficiently capturing the deployment environment and fine-tuning policies within the reconstructed scenes. Our method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to bridge the gap between realistic scene capture and effective training environments. Using iPhone-captured deployment scenes, we reconstruct meshes via GS, enabling training in settings that closely approximate real-world conditions. We conduct a comprehensive analysis of training strategies, pre-training datasets, and mesh reconstruction techniques, evaluating their impact on sim-to-real predictivity in real-world scenarios. Experimental results demonstrate that agents fine-tuned with EmbodiedSplat outperform both zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and synthetically generated datasets (HSSD), achieving absolute success rate improvements of 20% and 40% on real-world Image Navigation task. Moreover, our approach yields a high sim-vs-real correlation (0.87–0.97) for the reconstructed meshes, underscoring its effectiveness in adapting policies to diverse environments with minimal effort.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

  <!-- End paper abstract -->

  <!-- EmbodiedSplat Pipeline -->
<section class="section hero is-small">
    <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The EmbodiedSplat Pipeline</h2>
        <div class="content has-text-justified">
          <img src="static/images/EmbodiedSplat_Architecture.png" alt="EmbodiedSplat Pipeline" class="is-centered">
          <p>
            The EmbodiedSplat Pipeline involves capturing University scenes using <a href="https://poly.cam">Polycam</a> and <a href="https://docs.nerf.studio">Nerfstudio</a> which produces RGB frames, associated iPhone GT depth maps, and poses. <a href="https://maturk.github.io/dn-splatter/">DN-Splatter</a> is used to train Gaussian Splatting using depth and normal regularization. Meshes are extracted through Poisson Reconstruction from the trained gaussians and are processed and loaded into <a href="https://aihabitat.org">Habitat-Sim</a> for agent training in simulation. The trained policies are deployed in the same real-world scenes for image-goal navigation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

  <!-- End EmbodiedSplat Pipeline -->


  <!-- Section for Dataset -->

  <section class="section hero is-small">
    <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Our Dataset</h2>
        <div class="content has-text-justified">
          <!-- <img src="static/images/dataset.png" alt="Reconstructed meshes of the Captured scenes" class="is-centered"> -->
<div class="columns is-multiline is-centered">
            
            <div class="column is-half">
              <h3 class="title is-5">"lounge"</h3>
              <model-viewer src="static/meshes/grad_lounge_polycam_mesh.glb"
                            alt="Lounge Mesh"
                            camera-controls auto-rotate
                            shadow-intensity="0.8"
                            style="width:100%; height:300px;">
              </model-viewer>
            </div>

            <div class="column is-half">
              <h3 class="title is-5">"classroom"</h3>
              <model-viewer src="static/meshes/clough_classroom_polycam_mesh.glb"
                            alt="Classroom Mesh"
                            camera-controls auto-rotate
                            shadow-intensity="0.8"
                            style="width:100%; height:300px;">
              </model-viewer>
            </div>

          </div>
          <div class="columns is-multiline is-centered">
            
            <div class="column is-half">
              <h3 class="title is-5">"conf a"</h3>
              <model-viewer src="static/meshes/piedmont_metric3d_v2_mono_normal_gt_depth.glb"
                            alt="Conf a mesh"
                            camera-controls auto-rotate
                            shadow-intensity="0.8"
                            style="width:100%; height:300px;">
              </model-viewer>
            </div>

            <div class="column is-half">
              <h3 class="title is-5">"conf b"</h3>
              <model-viewer src="static/meshes/castleberry_metric3d_v2_mono_normal_gt_depth.glb"
                            alt="Conf b Mesh"
                            camera-controls auto-rotate
                            shadow-intensity="0.8"
                            style="width:100%; height:300px;">
              </model-viewer>
            </div>

          </div>
          <p>
            We capture scenes from a university environment (classroom, community lounges, conference rooms, etc). For custom data collection, we use an iPhone 13 Pro Max to record the iPhone RGB-D data using the <a href="https://poly.cam">Polycam</a>. Subsequently, we use <a href="https://docs.nerf.studio">Nerfstudio</a> to process the RGB-D data and sample 1000 aligned RGB-depth frames with low blur scores and corresponding poses. Each capture requires 20-30 minutes of recording with Polycam. We repeat this process for different indoor scenes, such as lounge, classroom, conf_a, and conf_b. For mesh reconstruction, we use <a href="https://maturk.github.io/dn-splatter/">DN-Splatter</a> as our method of choice for its superior performance on mesh reconstruction in comparison to others, and simplicity of its integration with <a href="https://aihabitat.org">Habitat-Sim</a>. DN-Splatter leverages depth-normal regularization, and smoothness losses to maintain geometric consistency during Gaussian Splat training. After training GS and converting into ply meshes, we convert the meshes to glb in Blender. The final meshes are shown above.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

  <!-- End of Dataset -->


    <!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Zero-Shot Evaluation Results</h2>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">

        <img src="static/images/zero_shot_results_combined_scenes_hm3d.png" alt="HM3D zero-shot results"/>
        <h2 class="subtitle has-text-centered">
          The results of zero-shot evaluation across individual scenes for the HM3D pre-trained policy. The policy demonstrates relatively high success rates in smaller scenes, such as conf_a and conf_b. However, it struggles in larger environments such as classroom and lounge, which are more complex and differ more significantly from the training data. The policy performs generally equally well in both DN and Polycam meshes, yielding similar success rates (~60%). Three MuSHRoom scenes are included in the evaluation. Having no Polycam meshes, we conduct evaluations on DN meshes only. Again, performances in sauna and honka are better than that in activity, suggesting that the policy is more effective in smaller, less complex environments. 
        </h2>
      </div>
      <div class="item">

        <img src="static/images/zero_shot_results_combined_scenes_hssd.png" alt="HSSD zero-shot results"/>
        <h2 class="subtitle has-text-centered">
          The results of zero-shot evaluation across individual scenes for the HSSD pre-trained policy. The policy reveals similar trend as the HM3D pre-trained policy, though with significantly lower success rates overall. This degradation in performance can be attributed to the synthetic nature of HSSD scenes, which lack the realism and scale necessary for effective generalization. And the performances on DN and Polycam meshes are still comparable.
        </h2>
      </div>
      </div>
     </div>
  </div>
</div>
</div>
</section>
  <!-- End image carousel -->

      <!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Fine-tuned Evaluation Results</h2>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/finetune_results_all_scenes_hm3d.png" alt="HM3D fine-tuned results"/>
        <h2 class="subtitle has-text-centered">
          We fine-tune the pre-trained policy on the training episodes for a single scene and evaluate on the corresponding validation episodes. We observe that fine-tuning significantly improves performance across all tested scenes. For the pre-trained HM3D policy, fine-tuning for 20M steps results in success rates approaching 90%+ across different meshes.
        </h2>
      </div>
      <div class="item">

        <img src="static/images/finetune_results_all_scenes_hssd.png" alt="HSSD fine-tuned results"/>
        <h2 class="subtitle has-text-centered">
          Similarly, for the HSSD pre-trained policy, fine-tuning leads to substantial improvements, with most policies achieving success rates of 80%+ in respective scenes. In particular, performance gains are particularly pronounced in larger, more complex environments such as classroom and lounge, which differ significantly from apartment-style scenes in HM3D and HSSD.
        </h2>
      </div>
      </div>
     </div>
  </div>
</div>
</div>
</section>
  <!-- End image carousel -->

    <!-- Real world results -->
<section class="section hero is-small">
    <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Real World Performance</h2>
        <div class="content has-text-justified">
          <img src="static/images/real_world_performance_results.png" alt="Real World Performance Results" class="is-centered">
          <p>
            We evaluate both zero-shot and fine-tuned policies in the real-world lounge scene on a Stretch robot. During the evaluation, each episode is capped at 100 steps, with 10 distinct start-and-goal locations sampled within the scene. To assess performance, we record the number of steps taken to reach the goal and the final distance to the goal at the end of each episode, determining whether the agent successfully completes the task. The zero-shot HM3D policy achieves a 50% success rate, demonstrating our hypothesized lack of generalization. We attribute this to the structural and semantic differences betweeen the lounge and the apartment-style scenes typically encountered in the HM3D dataset. Fine-tuning on the POLYCAM and DN mesh reconstructions of this scene improves performance, with success rates increasing to 70%. For HSSD, zero-shot performance is significantly lower at 10%, while fine-tuned policies improve success rates to 50% with POLYCAM and 40% with DN mesh. The improvements in evaluation performance on DN and POLYCAM meshes in simulation translate to improved real-world performance. This demonstrates that our approach can efficiently adapt policies to novel real-world environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

  <!-- End Real world results -->

        <!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Ablations and Analysis</h2>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/overfitted_combined_scenes_barchart.png" alt="Overfitted Results on all scenes"/>
        <h2 class="subtitle has-text-centered">
          We investigate the necessity of pretraining on large scale datasets by overfitting policies directly on POLYCAM and DN meshes with a policy trained from scratch (not pre-trained on large-scale datasets such as HM3D or HSSD) for 300M steps. As expected, the overfitted policies achieve near-perfect performance in simulation, maintaining high success rates. We further evaluate these overfitted policies in the real- world lounge scene using POLYCAM and DN meshes. Surprisingly, the overfitted policy trained on the POLYCAM mesh achieves a 50% success rate in real-world evaluations. In contrast, the policy trained on the DN mesh achieves only 10% success. This result suggests that our mesh-based training approach can yield non-zero real-world perfor- mance, even without large-scale pre-training. We attribute the performance gap between POLYCAM and DN-trained policies to differences in visual fidelity—POLYCAM meshes preserve more visual detail by directly utilizing original im- ages to reconstruct the scene, whereas DN meshes are based on GS which use the learned colors for the 3D Gaussians. 
        </h2>
      </div>

      <div class="item">

        <img src="static/images/combined_plot.png" alt="correlation between zero-shot success rates with PSNR and average distance to goal"/>
        <h2 class="subtitle has-text-centered">
          We illustrate the correlation between zero-shot validation success rates on DN meshes and two key factors: the Peak Signal-to-Noise Ratio (PSNR) of the corresponding 3D Gaussian Splats (GS) and the scale of the scene, measured as the average shortest distance between start-goal locations in validation episodes. We observe a negative correlation between success rate (SR) and average shortest distance—indicating that as the scale of the scene increases, the zero-shot success rate declines. Conversely, a positive correlation is seen between SR and PSNR, where higher validation PSNR values correspond to improved success rates. Notably, different trend lines emerge for MuSHRoom captures and our own Captured scenes. MuSHRoom captures generally exhibit higher validation PSNRs, likely due to the use of a stabilized gimbal during data collection.
        </h2>
      </div>

      <div class="item">

        <img src="static/images/zero_shot_success_rate_line_chart_hm3d.png" alt="HM3D zero-shot success rate"/>
        <h2 class="subtitle has-text-centered">
          We present average zero-shot success rates across HM3D validation set, POLYCAM meshes, and DN meshes at various stages of HM3D policy pre-training. This analysis aims to determine whether continuous performance improvements on HM3D validation scenes translate to improved zero-shot performance on our Captured scenes. We observe that while performance initially increases, it begins to deteriorate or plateau at approximately 400M steps, despite continued improvements on HM3D validation scenes.
        </h2>
      </div>

      <div class="item">

        <img src="static/images/zero_shot_success_rate_line_chart_hssd.png" alt="HSSD zero-shot success rate"/>
        <h2 class="subtitle has-text-centered">
          We show a similar experiment conducted using the HSSD dataset. Up to 300M steps, improvements in HSSD validation performance correspond to slight improvements in zero-shot performance on our Captured scenes. However, performance plateaus for Captured, showing no further gains despite continued improvement on HSSD validation scenes. For HM3D pre-training, POLYCAM meshes outperform DN meshes in success rates, while for HSSD, the trend reverses slightly, highlighting the impact of pre-training dataset characteristics on zero-shot generalization.
        </h2>
      </div>
      </div>
     </div>
  </div>
</div>
</div>
</section>
  <!-- End image carousel -->

    <!-- Conclusion -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>
            In this work, we introduced a comprehensive pipeline for bridging the gap between real-world and simulated environments in training embodied agents using 3D Gaussian Splats (GS) and Polycam. By leveraging the <a href="https://xuqianren.github.io/publications/MuSHRoom/">MuSHRoom</a> dataset and custom iPhone-captured scenes, we demonstrated an efficient and scalable approach to policy personalization, leveraging 3D scene reconstruction from low-effort collects, enabling high-quality training for the ImageNav task. Our pipeline facilitates accessible and replicable scene collection without requiring specialized hardware or significant costs, making it a practical solution for large-scale embodied AI research.
We evaluated overfitted, zero-shot, and fine-tuned policies, showing that fine-tuning pre-trained policies on real-world scene reconstructions improves sim-to-real transfer. We also analyzed the differences between GS-generated DN meshes and POLYCAM meshes, finding that POLYCAM more closely resemble real-world scenes.

This work lays the foundation for seamlessly integrating real-world scene captures into simulation, expanding the applicability of embodied AI systems. Another promising avenue is integrating GS directly into training, replacing visual observations and goal images to enhance learning efficiency. Furthermore, we aim to extend the use of Gaussian Splats to more complex embodied AI tasks, such as rearrangement and mobile manipulation, broadening its impact across diverse real-world applications.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

  <!-- End Conclusion -->


  <!-- Image carousel -->
  <!--
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">

        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">

        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">

        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">

      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
-->
  <!-- End image carousel -->




  <!-- Youtube video -->
  <!--
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
-->
  <!-- End youtube video -->


  <!-- Video carousel -->
  <!--
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
-->
  <!-- End video carousel -->






  <!-- Paper poster -->
  <!--
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
-->
  <!--End paper poster -->


  <!--BibTex citation -->
  <!--
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section>
-->

  <!-- <section class="section">
    <div class="container is-max-desktop content">
      Coming Soon!
    </div>
  </section> -->
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
